{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import math\n","from typing import Optional, List\n","import torch\n","from torch import nn\n","from labml import tracker"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class PrepareForMultiHeadAttention(nn.Module): # nn.Module\n","    def __init__(self, d_model, heads, d_k, bias): \n","        '''\n","            var 'heads' is the number of heads \n","            var 'd_model' is the number of features in q, k, v\n","            var 'd_k' is the demention of each head\n","            var bias is ?\n","        '''\n","        super().__init__()\n","        self.linear = nn.Linear(d_model, heads*d_k, bias = bias)\n","        self.heads = heads\n","        self.d_k = d_k\n","\n","    def forward(self, x: torch.Tensor):\n","        '''\n","            var x is the vector of all\n","        '''\n","        head_shape = x.shape[:-1] #??? #输入的形状为[seq_len, batch_size, d_model] 或[batch_size, d_model] 。我们对最后一维应用线性变换，并将其分为多个头。\n","        x = self.linear(x) #???\n","        x = x.view(*head_shape, self.heads, self.d_k) #???\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiheadAttention(nn.Module):\n","    def __init__(self, heads, d_model, dropout_prob=0.1, bias=True):\n","        super().__init__()\n","        self.d_k = d_model\n","        self.heads = heads\n","        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n","        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n","        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True) \n","        self.softmax = nn.Softmax(dim=1) #dim=1能保证在时间维度上对key进行注意力softmax？\n","        self.output = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.scale = 1/math.sqrt(self.d_k)\n","        self.attn = None\n","    def get_scores(self, query, key):\n","        return torch.einsum('ibhd, jbhd->ijbh', query, key)\n","    def prepare_mask(self, mask, query_shape, key_shape):\n","        assert mask.shape[0]==1 or mask.shape[0]==query_shape[0]\n","        assert mask.shape[1]==key_shape[0]\n","        assert mask.shape[2]==1 or mask.shape[2]==query_shape[1]\n","        mask = mask.unsqueeze(-1) #???\n","        return mask\n","    def forward(self, *, query, key, value, mask):\n","        seq_len, batch_size, _ = query.shape\n","        if mask is not None:\n","            mask = self.prepare_mask(mask, query.shape, key.shape)\n","        query = self.query(query)\n","        key = self.key(key)\n","        value = self.value(value)\n","        scores = self.get_scores(query, key)\n","        scores *= self.scale\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0, float('-inf'))\n","        attn = self.softmax(scores)\n","        tracker.debug('attn', attn)\n","        attn = self.dropout(attn)\n","        x = torch.einsum('ijbh,jbhd->ibhd', attn, value)\n","        self.attn = attn.detach()\n","        x = x.reshape(seq_len, batch_size, -1)\n","        return self.output(x)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.4"}},"nbformat":4,"nbformat_minor":2}
